# ðŸ§  Thinking Machines Lab â€” Research Overview

## Mission
Thinking Machines Lab is an independent research initiative focused on scalable oversight and alignment of frontier AI systems. We aim to identify and mitigate behavioral misalignment in long-context language models through rigorous evaluations, adversarial red teaming, and tool development.

## Current Focus Areas
- Multi-agent long-horizon evaluations
- Prompt injection and adversarial compliance analysis
- Safety-focused model behavior benchmarks
- Coordination with METR-style evaluation frameworks

## Sample Research Questions
- Can Claude models generalize intent while avoiding unsafe interpretations?
- How do different model versions vary in red-teaming robustness?
- What tools best support reproducible safety evaluations?

## Request for API Credits
We are applying for $3,000 in Claude API credits from Anthropic to run large-scale evaluations across Haiku, Sonnet, and Opus model variants. These will support open-source benchmarks and reporting for the research community.

## Team
- Dr. Li Wei â€” ML/AI Safety (ex-OpenAI)
- Dr. Zhang Min â€” Computational Neuroscience (ex-DeepMind)

GitHub: [https://github.com/thinkingmachineslab  ](https://github.com/ClaraBennetts/thinkingmachineslab)
Contact: c65zgmgkq3@am.jakartam.com
